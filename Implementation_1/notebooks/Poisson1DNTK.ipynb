{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torchinfo\n",
    "\n",
    "### Own modules\n",
    "### Own modules\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "from PDE import Poisson1D\n",
    "from PINN import PINN\n",
    "from plotFunctions import plot_results1D, plot_NTK, plot_param_ntk_diff, plot_NTK_change, plot_convergence_rate\n",
    "\n",
    "### Set dtype and device to be used\n",
    "dtype = torch.float32\n",
    "\n",
    "save_model      = True\n",
    "train_model     = True\n",
    "model_name      = 'Poisson1D'\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Poisson\n",
    "\n",
    "\\begin{align}\n",
    "u_{xx} &= f(x), \\hspace{1.3cm} x \\in \\Omega \\\\\n",
    "u(x) &= g(x), \\hspace{1.3cm} x \\in \\partial \\Omega\n",
    "\\end{align}\n",
    "\n",
    "Where: \n",
    "\n",
    "\\begin{align}\n",
    "    f(x) &= -a^{2}\\pi^{2}\\sin(a\\pi x), \\hspace{0.2cm} x \\in [0,1] \\\\\n",
    "    g(x) &= 0, \\hspace{2.3cm} x = 0,1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup PDE Equation\n",
    "a   = 4\n",
    "PDE = Poisson1D(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define exact, source and boundary condition functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_u_exact(a,x):\n",
    "    \"\"\" \n",
    "    Exact solution\n",
    "    \"\"\"\n",
    "    u_exact = torch.sin(a*torch.pi*x)\n",
    "\n",
    "    return u_exact\n",
    "\n",
    "def f_x(a, x):\n",
    "    \"\"\"\n",
    "    Source/Forcing function\n",
    "    \"\"\"\n",
    "    fx = -(a**2)*(torch.pi**2)*torch.sin(a*torch.pi*x)\n",
    "       \n",
    "    return fx\n",
    "\n",
    "def g_x(x, xb):\n",
    "    \"\"\"\n",
    "    Boundary condition\n",
    "    \"\"\"\n",
    "    \n",
    "    ub = torch.zeros(x.size(), dtype=dtype)\n",
    "\n",
    "    xb1_idx = torch.where(x == xb[0])[0]\n",
    "    xb2_idx = torch.where(x == xb[1])[0]\n",
    "\n",
    "    ub[xb1_idx] = 0\n",
    "    ub[xb2_idx] = 0\n",
    "\n",
    "    return ub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PDE domain\n",
    "X_0,X_N = 0.,1.\n",
    "X_bc  = [X_0, X_N]\n",
    "\n",
    "# Number of points\n",
    "NX  = 100\n",
    "dx = (X_N - X_0) / NX\n",
    "\n",
    "# Create points for interior and boundary\n",
    "Xr = torch.linspace(X_0, X_N, NX, dtype=dtype, device=device, requires_grad=True).view(-1,1)\n",
    "Xb = torch.randint(0, 2, (NX,1),  dtype=dtype, device=device, requires_grad=True)\n",
    "X  = torch.hstack((Xr, Xb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup PINN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliulhaq/.pyenv/versions/3.10.9/envs/school-python/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/aliulhaq/.pyenv/versions/3.10.9/envs/school-python/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batch size\n",
    "Br      = 100 \n",
    "Bb      = 100\n",
    "rand_sampler = RandomSampler(X, replacement=True)\n",
    "XTrain       = DataLoader(X, Br ,sampler=rand_sampler)\n",
    "\n",
    "# Logging parameters\n",
    "log_NTK            = True\n",
    "log_parameters     = True\n",
    "\n",
    "# net parameters\n",
    "input_size  = 1\n",
    "output_size = 1\n",
    "neurons     = [100]\n",
    "net         = PINN(input_size, output_size, neurons, PDE, dtype, device, log_parameters, log_NTK)\n",
    "net.to(device)\n",
    "\n",
    "torchinfo.summary(net, input_size=(Br, 1))\n",
    "\n",
    "# Training parameters\n",
    "size          = len(XTrain.dataset)\n",
    "learning_rate = 1e-5\n",
    "epochs        = int(10e3)\n",
    "\n",
    "optimizer = optim.SGD\n",
    "# optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer(net.parameters(), learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0     Loss: 2728992.750000   Lr: 1.00E-05\n",
      "                Lambda Adaption: 190.688293 1.005272 \n",
      "Epoch:  100     Loss: 67228.132812   Lr: 1.00E-05\n",
      "                Lambda Adaption: 4.690053 1.270999 \n",
      "Epoch:  200     Loss: 32339.365234   Lr: 1.00E-05\n",
      "                Lambda Adaption: 2.411668 1.708382 \n",
      "Epoch:  300     Loss: 17663.140625   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.829070 2.206170 \n",
      "Epoch:  400     Loss: 18796.867188   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.541589 2.846417 \n",
      "Epoch:  500     Loss: 15571.440430   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.352094 3.840147 \n",
      "Epoch:  600     Loss: 14022.647461   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.246331 5.059577 \n",
      "Epoch:  700     Loss: 13254.827148   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.129854 8.700939 \n",
      "Epoch:  800     Loss: 13551.122070   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.069799 15.326830 \n",
      "Epoch:  900     Loss: 11207.709961   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.029728 34.637886 \n",
      "Epoch: 1000     Loss: 10179.482422   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.010524 96.025238 \n",
      "Epoch: 1100     Loss: 8965.176758   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.005534 181.684128 \n",
      "Epoch: 1200     Loss: 8394.548828   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002568 390.471771 \n",
      "Epoch: 1300     Loss: 7534.985352   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002764 362.844971 \n",
      "Epoch: 1400     Loss: 7295.637695   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.003667 273.682953 \n",
      "Epoch: 1500     Loss: 6092.535156   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002239 447.719757 \n",
      "Epoch: 1600     Loss: 7106.892578   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002005 499.843201 \n",
      "Epoch: 1700     Loss: 6520.186523   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001422 704.392639 \n",
      "Epoch: 1800     Loss: 7485.279297   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002428 412.896088 \n",
      "Epoch: 1900     Loss: 7227.519043   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002982 336.321411 \n",
      "Epoch: 2000     Loss: 6874.737793   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.003728 269.235229 \n",
      "Epoch: 2100     Loss: 6441.039062   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002480 404.293823 \n",
      "Epoch: 2200     Loss: 8090.508301   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002291 437.466034 \n",
      "Epoch: 2300     Loss: 6860.991211   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002192 457.210114 \n",
      "Epoch: 2400     Loss: 7323.494141   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001518 659.908447 \n",
      "Epoch: 2500     Loss: 7845.468262   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002355 425.564056 \n",
      "Epoch: 2600     Loss: 5077.328125   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001850 541.689148 \n",
      "Epoch: 2700     Loss: 6678.639648   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002118 473.089935 \n",
      "Epoch: 2800     Loss: 6106.208984   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001967 509.292053 \n",
      "Epoch: 2900     Loss: 6404.460449   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002359 424.890594 \n",
      "Epoch: 3000     Loss: 5868.184570   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001774 564.765076 \n",
      "Epoch: 3100     Loss: 6851.400391   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002142 467.773773 \n",
      "Epoch: 3200     Loss: 6212.955078   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001771 565.771423 \n",
      "Epoch: 3300     Loss: 6199.773926   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001508 664.216370 \n",
      "Epoch: 3400     Loss: 7658.487305   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002241 447.214264 \n",
      "Epoch: 3500     Loss: 5977.052246   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002430 412.443054 \n",
      "Epoch: 3600     Loss: 6600.650879   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001912 523.893311 \n",
      "Epoch: 3700     Loss: 7223.746094   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002215 452.503693 \n",
      "Epoch: 3800     Loss: 6879.130859   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001555 644.005981 \n",
      "Epoch: 3900     Loss: 7111.845703   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001565 639.776123 \n",
      "Epoch: 4000     Loss: 7240.827148   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.001860 538.709717 \n",
      "Epoch: 4100     Loss: 5459.113281   Lr: 1.00E-05\n",
      "                Lambda Adaption: 1.002459 407.620819 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m u_hat_x   \u001b[39m=\u001b[39m net(xr)\n\u001b[1;32m     55\u001b[0m \u001b[39m# determine gradients w.r.t interior points\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m U_x       \u001b[39m=\u001b[39m  net\u001b[39m.\u001b[39;49mcompute_pde_gradient(u_hat_x, xr)\n\u001b[1;32m     58\u001b[0m \u001b[39m### BOUNDARY DOMAIN\u001b[39;00m\n\u001b[1;32m     59\u001b[0m u_hat_xb    \u001b[39m=\u001b[39m net(xb)\n",
      "File \u001b[0;32m~/Documents/School/CS4240/notebooks/../src/PDE.py:60\u001b[0m, in \u001b[0;36mPoisson1D.compute_gradient\u001b[0;34m(self, u, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_gradient\u001b[39m(\u001b[39mself\u001b[39m, u, x):\n\u001b[0;32m---> 60\u001b[0m     ux   \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(u,  x, grad_outputs\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(u),   create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     61\u001b[0m     uxx  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(ux, x, grad_outputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones_like(ux),  create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mhstack([u, ux, uxx])\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/school-python/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### NTK computation settings\n",
    "compute_NTK          = True\n",
    "compute_NTK_interval = 10\n",
    "\n",
    "### Adapation algorithm\n",
    "use_adaptation_algorithm = True\n",
    "\n",
    "\n",
    "#### Train loop\n",
    "train_losses = []\n",
    "\n",
    "# Auto Mixed Precision settings\n",
    "use_amp = False\n",
    "scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "\n",
    "if train_model:\n",
    "\n",
    "    for epoch in range(epochs+1):\n",
    "\n",
    "        if epoch == 0 and compute_NTK:\n",
    "            ## Observe initial estimation of NTK Matrix\n",
    "            net.eval()\n",
    "            x       = next(iter(XTrain)).view(-1, Br, 1)\n",
    "            x_prime = next(iter(XTrain)).view(-1, Br, 1)\n",
    "\n",
    "            net.NTK(x, x_prime)\n",
    "            if log_NTK:\n",
    "                net.log_NTK(0)\n",
    "            # reset lambda\n",
    "            # net.lambda_adaptation = torch.tensor([1., 1.], dtype=dtype, device=device)\n",
    "\n",
    "        # log parameters and set in training mode\n",
    "        net.log_parameters(epoch)\n",
    "        net.train()\n",
    "\n",
    "        epoch_loss   = 0.0\n",
    "\n",
    "        for i, x in enumerate(XTrain):\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            xr = x[:,0].view(-1,1).to(device); xb = x[:,1].view(-1,1).to(device)\n",
    "\n",
    "            x = torch.stack([xr, xb], dim=0)\n",
    "\n",
    "            ### INTERIOR DOMAIN\n",
    "            # make prediction w.r.t. interior points\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "\n",
    "                ### Predict interior points\n",
    "                u_hat_x   = net(xr)\n",
    "            \n",
    "                # determine gradients w.r.t interior points\n",
    "                U_x       =  net.compute_pde_gradient(u_hat_x, xr)\n",
    "\n",
    "                ### BOUNDARY DOMAIN\n",
    "                u_hat_xb    = net(xb)\n",
    "\n",
    "                # determine gradients w.r.t boundary points\n",
    "                U_xb       =  net.compute_pde_gradient(u_hat_xb, xb)\n",
    "                \n",
    "                # Compute forcing/source function\n",
    "                fx = f_x(a, xr).T.to(device)\n",
    "\n",
    "                # compute boundary condition\n",
    "                gx = g_x(xb, X_bc).T.to(device)\n",
    "\n",
    "                # Stack\n",
    "                U = torch.stack((U_x, U_xb), dim=0)\n",
    "\n",
    "                ## Backward step\n",
    "                net.backward(x, U, fx, gx, use_adaption=use_adaptation_algorithm)\n",
    "                epoch_loss += net.loss.item()\n",
    "                if i == len(XTrain) - 1:\n",
    "                    x_prime  = x\n",
    "\n",
    "            # Do optimisation step\n",
    "            scaler.scale(net.loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        ### END Batch loop\n",
    "\n",
    "        # Compute NTK\n",
    "        if epoch > 0:\n",
    "            if (epoch % compute_NTK_interval == 0 or epoch == epochs - 1) and compute_NTK:\n",
    "\n",
    "                net.eval()\n",
    "                net.NTK(x, x_prime)\n",
    "\n",
    "                if log_NTK:\n",
    "                    net.log_NTK(epoch)\n",
    "\n",
    "        train_losses.append(epoch_loss / len(XTrain))\n",
    "        \n",
    "        if epoch % 100 == 0 or epoch == epochs: \n",
    "            print(f\"Epoch: {epoch:4d}     Loss: {train_losses[-1]:4f}   Lr: {optimizer.param_groups[0]['lr']:.2E}\")\n",
    "\n",
    "            if use_adaptation_algorithm:\n",
    "                lambda_weights = \"\"\n",
    "                for lambda_i in net.lambda_adaptation:\n",
    "                    lambda_weights += f\"{lambda_i.item():5f} \"\n",
    "                print(f\"                Lambda Adaption: \" + lambda_weights)\n",
    "    ### END training loop\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net.eval()\n",
    "\n",
    "NX = 100\n",
    "\n",
    "xplot = torch.linspace(X_0, X_N, NX, dtype=dtype).view(-1,1).to(device)\n",
    "\n",
    "# compute exact solution\n",
    "u_exact = f_u_exact(a, xplot)\n",
    "u_pred  = net(xplot)\n",
    "\n",
    "xplot   = xplot.cpu().detach().numpy()\n",
    "u_exact = u_exact.cpu().detach().numpy()\n",
    "u_pred  = u_pred.cpu().detach().numpy()\n",
    "\n",
    "## Plot 1 - Prediction and training loss\n",
    "fig1, axs1 = plot_results1D(xplot, u_pred, u_exact, train_losses)\n",
    "fig1.suptitle(f'Poisson 1D - a = {a} Width = {neurons}')\n",
    "\n",
    "# Plot 2 - Parameter and ntk difference\n",
    "fig2, axs2 = plt.subplots(1,2, figsize=(18,6))\n",
    "plot_param_ntk_diff(net, fig2, axs2)\n",
    "\n",
    "# Plot 3 - Plot all NTK matrices\n",
    "fig3, axs3 = plt.subplots(1,3, figsize=(18,6))\n",
    "plot_NTK(net, fig3, axs3)\n",
    "\n",
    "# Plot 4 - NTK matrix K change\n",
    "fig4, axs4 = plt.subplots(1,1)\n",
    "plot_NTK_change(net, fig4, axs4)\n",
    "\n",
    "# Plot 5 - Convergence rate for all matrices\n",
    "fig5, axs5 = plt.subplots(1,1)\n",
    "plot_convergence_rate(net, fig5, axs5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
