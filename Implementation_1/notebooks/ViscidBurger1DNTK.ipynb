{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViscidBurger 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torchinfo\n",
    "\n",
    "### Own modules\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "from PDE import ViscidBurger1D\n",
    "from PINN import PINN\n",
    "from plotFunctions import plot_results2D, plot_NTK, plot_param_ntk_diff, plot_NTK_change, plot_convergence_rate\n",
    "\n",
    "### Set dtype and device to be used\n",
    "dtype = torch.float32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Viscid1D Exact, forcing function and boundary condition\n",
    "def u_exact():\n",
    "    \"\"\" \n",
    "    Exact solution\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat('../Data/burgers_shock.mat')\n",
    "\n",
    "    t = data['t'].flatten()[:,None]\n",
    "    x = data['x'].flatten()[:,None]\n",
    "    u_exact = np.real(data['usol'])\n",
    "\n",
    "    return x, t, u_exact\n",
    "\n",
    "def f_x(x,t):\n",
    "    \"\"\"\n",
    "    Source/Forcing function\n",
    "    \"\"\"\n",
    "\n",
    "    fx = torch.zeros( (1, x.shape[0]), dtype=dtype, device=device)\n",
    "       \n",
    "    return fx\n",
    "\n",
    "def g_x(x, t, xb, t_ic):\n",
    "    \"\"\"\n",
    "    Boundary condition\n",
    "    \"\"\"\n",
    "    \n",
    "    ub = torch.zeros((1, x.shape[0]), dtype=dtype, device=device)\n",
    "\n",
    "    # check for boundary condition\n",
    "    xb1_idx = torch.where(x == xb[0])[0]\n",
    "    xb2_idx = torch.where(x == xb[1])[0]\n",
    "\n",
    "    # assert boundary condition\n",
    "    ub[:,xb1_idx] = 0\n",
    "    ub[:,xb2_idx] = 0\n",
    "\n",
    "    return ub\n",
    "\n",
    "def h_x(x, t, t_ic):\n",
    "    \"\"\"\n",
    "    Initial condition\n",
    "    \"\"\"\n",
    "    u_ic = -torch.sin(torch.pi*x)\n",
    "\n",
    "    return u_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "def latin_hypercube(X_0, X_N, N):\n",
    "    sampler = qmc.LatinHypercube(d=1)\n",
    "    sample = sampler.random(n=N).reshape(-1,1)\n",
    "    sample = qmc.scale(sample, X_0, X_N)\n",
    "\n",
    "    if dtype == torch.float32:\n",
    "        sample = sample.astype(np.float32)\n",
    "\n",
    "    sample = torch.from_numpy(sample).requires_grad_(True).to(device)\n",
    "\n",
    "    return sample\n",
    "\n",
    "def create_bc_points(N, lb, up):\n",
    "    Xb = torch.cat( [lb*torch.ones((N//2, 1), dtype=dtype), up*torch.ones((N//2, 1), dtype=dtype)] ).to(device).requires_grad_(True)\n",
    "    return Xb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup PDE Equation\n",
    "nu  = 0.01/torch.pi\n",
    "PDE = ViscidBurger1D(nu)\n",
    "\n",
    "# Define PDE domain\n",
    "X_0, X_N = -1., 1.\n",
    "T_0, T_N =  0,  1.\n",
    "t_ic     = T_0\n",
    "\n",
    "X_bc     = [X_0, X_N]\n",
    "\n",
    "# Number of points for interior, boundary and inital condition\n",
    "NX = int(512)\n",
    "Nb = int(512)\n",
    "Ni = int(512)\n",
    "\n",
    "Xr = latin_hypercube(X_0, X_N, NX)\n",
    "T  = latin_hypercube(T_0, T_N, NX)\n",
    "\n",
    "Xb = create_bc_points(Nb, X_0, X_N)\n",
    "Tb = latin_hypercube(T_0, T_N, Nb)\n",
    "\n",
    "Xi = latin_hypercube(X_0, X_N, Ni)\n",
    "Ti = torch.zeros((Ni,1), dtype=dtype, requires_grad=True, device=device)\n",
    "\n",
    "X_r     = torch.hstack([Xr, T])\n",
    "X_bc_ic = torch.hstack([Xb, Tb, Xi, Ti] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PINN\n",
    "\n",
    "### Settings\n",
    "\n",
    "# save model\n",
    "train_model     = True\n",
    "save_model      = False\n",
    "model_name      = 'ViscidBurger1D'\n",
    "\n",
    "# NTK computation settings\n",
    "compute_NTK          = True\n",
    "compute_NTK_interval = 100\n",
    "\n",
    "# Logging parameters\n",
    "log_parameters     = True\n",
    "log_NTK            = True\n",
    "\n",
    "# Adapation algorithm\n",
    "use_adaptation_algorithm = True\n",
    "\n",
    "# correct for coupled parameters\n",
    "if not compute_NTK:\n",
    "    use_adaptation_algorithm = False\n",
    "if not compute_NTK:\n",
    "    log_NTK = False\n",
    "if not compute_NTK:\n",
    "    use_adaptation_algorithm = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "\n",
    "# Batch size\n",
    "Br = 512\n",
    "Bb = 512\n",
    "Bi = 512\n",
    "rand_sampler1 = RandomSampler(X_r, replacement=True)\n",
    "XTrain        = DataLoader(X_r, batch_size=Br ,sampler=rand_sampler1)\n",
    "\n",
    "rand_sampler2 = RandomSampler(X_bc_ic, replacement=True)\n",
    "XTrain_bc_ic  = DataLoader(X_bc_ic, batch_size=Bb, sampler=rand_sampler2)\n",
    "\n",
    "training_batches    = len(XTrain)\n",
    "training_batches_xb = len(XTrain_bc_ic)\n",
    "\n",
    "print(f\"Training batch XTrain: {training_batches} \\nTraining batch XTrain_bc: {training_batches_xb}\")\n",
    "\n",
    "\n",
    "# Create network\n",
    "input_size  = 2\n",
    "output_size = 1\n",
    "neurons     = [20, 20, 20, 20, 20, 20, 20]\n",
    "# neurons     = [1000]\n",
    "\n",
    "init_type = 'xavier' # initialisation type for the weights\n",
    "\n",
    "net         = PINN(input_size, output_size, neurons, PDE, init_type,  dtype, device, log_parameters, log_NTK)\n",
    "net.to(device)\n",
    "\n",
    "if dtype == torch.float64:\n",
    "    net.double()\n",
    "\n",
    "torchinfo.summary(net, input_size=(Br, 2), dtypes=[dtype], device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameter\n",
    "learning_rate = 1e-5\n",
    "epochs        = int(10e3)\n",
    "optimizer = optim.SGD\n",
    "# optimizer = optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Train network\n",
    "optimizer = optimizer(net.parameters(), learning_rate)\n",
    "\n",
    "# Auto Mixed Precision settings\n",
    "use_amp = True\n",
    "if device == torch.device('cpu'):\n",
    "    print(\"Using CPU\")\n",
    "    use_amp = False\n",
    "\n",
    "# Use scaler\n",
    "scaler    = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.9)\n",
    "\n",
    "### Model save settings\n",
    "if use_adaptation_algorithm and compute_NTK:\n",
    "    model_adaption = '_adapted'\n",
    "else:\n",
    "    model_adaption = ''\n",
    "\n",
    "if isinstance(optimizer, optim.SGD):\n",
    "    opt = 'SGD'\n",
    "elif isinstance(optimizer , optim.Adam):\n",
    "    opt = 'Adam'\n",
    "\n",
    "file_name = f'{model_name}_Epoch={epochs}_Optimizer={opt}{model_adaption}'\n",
    "path      = './output/models/'\n",
    "pathfile  = path+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train loop\n",
    "train_losses = []\n",
    "\n",
    "if train_model:\n",
    "    ## Observe initial estimation of NTK Matrix\n",
    "    if compute_NTK:\n",
    "        print(\"Compute initial NTK estimation\\n\")\n",
    "        net.eval()\n",
    "\n",
    "        x       = next(iter(XTrain))\n",
    "        x_prime = next(iter(XTrain))\n",
    "\n",
    "        x_bc_ic         = next(iter(XTrain_bc_ic))\n",
    "        x_bc_ic_prime   = next(iter(XTrain_bc_ic))\n",
    "\n",
    "        xb          = x_bc_ic[:,[0,1]].view(-1,2);          xi          = x_bc_ic[:,[2,3]].view(-1,2)\n",
    "        xb_prime    = x_bc_ic_prime[:,[0,1]].view(-1,2);    xi_prime    = x_bc_ic_prime[:,[2,3]].view(-1,2)\n",
    "        \n",
    "        x       = [x, xb, xi]\n",
    "        x_prime = [x_prime, xb_prime, xi_prime]\n",
    "\n",
    "        net.NTK(x, x_prime)\n",
    "\n",
    "        if log_NTK:\n",
    "            net.log_NTK(0)\n",
    "            plot_NTK(net)\n",
    "            plt.show()\n",
    "\n",
    "        max_lr = 2/torch.max(torch.real(net.lambda_K))\n",
    "\n",
    "        if(learning_rate > max_lr):\n",
    "            print(f\"Learning step greater than max_NTK_lr: 2 / lambda_max, unstable training. Lower learning rate. lr= {learning_rate} max_lr: {max_lr.item()}\")\n",
    "        \n",
    "        # reset lambda\n",
    "        # net.lambda_adaptation = torch.Tensor([1., 1., 1.]).to(device)\n",
    "\n",
    "    ## Training loop\n",
    "    print(\"Start training\\n\")\n",
    "    start_epoch = time.time()\n",
    "    for epoch in range(epochs+1):\n",
    "        # log parameters and set in training mode\n",
    "        if log_parameters:\n",
    "            net.log_parameters(epoch)\n",
    "\n",
    "        # set in training mode\n",
    "        net.train()\n",
    "        epoch_loss = 0.0\n",
    "        for i, xr in enumerate(XTrain):\n",
    "            # asymmetrical training sets\n",
    "            x_bc_ic = next(iter(XTrain_bc_ic))\n",
    "\n",
    "            # set up training sets\n",
    "            xb = x_bc_ic[:,[0,1]].view(-1,2)\n",
    "            xi = x_bc_ic[:,[2,3]].view(-1,2)\n",
    "\n",
    "            x = [xr, xb, xi]\n",
    "\n",
    "            if i == len(XTrain) - 1 and epoch % compute_NTK_interval != 0:\n",
    "                x_prime  = x\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                ### INTERIOR Domain\n",
    "                u_hat_x     = net(xr)\n",
    "                # determine gradients w.r.t interior points\n",
    "                U_x         =  net.compute_pde_gradient(u_hat_x, xr)\n",
    "\n",
    "                ### BOUNDARY Domain\n",
    "                u_hat_xb    = net(xb)\n",
    "                # determine gradients w.r.t boundary points\n",
    "                U_xb        =  net.compute_pde_gradient(u_hat_xb, xb)\n",
    "                \n",
    "                ### INITIAL condition\n",
    "                u_hat_xi    = net(xi)\n",
    "                # determine gradients w.r.t initial condition points\n",
    "                U_xi        =  net.compute_pde_gradient(u_hat_xi, xi)    \n",
    "            \n",
    "                # Compute forcing/source function\n",
    "                fx = f_x(xr[:,0], xr[:,1])\n",
    "\n",
    "                # compute boundary condition\n",
    "                gx = g_x(xb[:,0], xb[:,1], X_bc, t_ic)\n",
    "\n",
    "                # compute initial condition\n",
    "                hx = h_x(xi[:,0], xi[:,1], t_ic)\n",
    "\n",
    "                # Stack\n",
    "                U = [U_x, U_xb, U_xi]\n",
    "\n",
    "                ## Backward step\n",
    "                net.backward(x, U, fx, gx, hx, use_adaption=use_adaptation_algorithm)\n",
    "                net.loss = net.loss / training_batches \n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(net.loss).backward()\n",
    "            else:\n",
    "                net.loss.backward()\n",
    "                #end\n",
    "\n",
    "            # Detach from graph to prevent it being accumulated in backward graph again\n",
    "            u_hat_x.detach_();  u_hat_xb.detach_();     u_hat_xi.detach_()\n",
    "            net.loss.detach_()\n",
    "\n",
    "            # Accumulate loss over batch\n",
    "            epoch_loss += net.loss\n",
    "\n",
    "            # Do optimization step after all gradients have been accumulated\n",
    "            if (i + 1) % training_batches == 0:\n",
    "                if use_amp:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                #end\n",
    "                optimizer.zero_grad()\n",
    "            #end\n",
    "        ### End Batch loop\n",
    "\n",
    "        train_losses.append(epoch_loss.item() / 2*training_batches )\n",
    "\n",
    "        # Compute NTK\n",
    "        if epoch > 0 and epoch < epochs - 1:\n",
    "            if (epoch % compute_NTK_interval == 0) and compute_NTK:\n",
    "            \n",
    "                net.eval()\n",
    "                net.NTK(x, x_prime)\n",
    "                if log_NTK:\n",
    "                    net.log_NTK(epoch)\n",
    "                    if save_model:                   \n",
    "                        net.save_log(pathfile)\n",
    "\n",
    "                max_lr = 2/torch.max(torch.real(net.lambda_K))\n",
    "\n",
    "                if(learning_rate > max_lr):\n",
    "                    print(f\"Learning step greater than max_NTK_lr: 2 / lambda_max, unstable training. Lower learning rate. lr= {learning_rate} max_lr: {max_lr.item()}\")\n",
    "            #end\n",
    "        #end\n",
    "\n",
    "        # Print epoch\n",
    "        if epoch % 100 == 0 or epoch == epochs: \n",
    "            end_epoch = time.time()\n",
    "            print(f\"Epoch: {epoch:4d}     Loss: {train_losses[-1]:5f}   Lr: {optimizer.param_groups[0]['lr']:.2E}       t: {(end_epoch - start_epoch):2f} [s]\")\n",
    "\n",
    "            if use_adaptation_algorithm:\n",
    "                lambda_weights = \"\"\n",
    "                for lambda_i in net.lambda_adaptation:\n",
    "                    lambda_weights += f\"{lambda_i.item():5f} \"\n",
    "                print(f\"                Lambda Adaption: \" + lambda_weights)\n",
    "            start_epoch = time.time()\n",
    "            #end\n",
    "        #end\n",
    "\n",
    "    ### End training loop\n",
    "\n",
    "    #### save model\n",
    "    if save_model:\n",
    "        net.save_model(pathfile)\n",
    "        net.save_log(pathfile)\n",
    "        with open(f'{pathfile}.npy', 'wb') as f:\n",
    "            np.save(f, np.array(train_losses))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get exact solution\n",
    "x, t, U_exact = u_exact()\n",
    "\n",
    "xplot = torch.from_numpy(x.astype(np.float32)).to(device)\n",
    "tplot = torch.from_numpy(t.astype(np.float32)).to(device)\n",
    "\n",
    "U_pred = []\n",
    "# compute prediction solution\n",
    "for t_i in tplot:\n",
    "    t_i = torch.ones(xplot.shape, dtype=dtype, device=device)*t_i\n",
    "    U_pred.append( net(torch.hstack((xplot, t_i))) )\n",
    "U_pred  = torch.hstack(U_pred)\n",
    "\n",
    "xplot   = xplot.cpu().detach().numpy()\n",
    "tplot   = tplot.cpu().detach().numpy()\n",
    "U_pred  = U_pred.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot 1 - Prediction and training loss\n",
    "xplot = [xplot, tplot]\n",
    "T_idxs = [0, 0.25, 0.5, 0.75]\n",
    "plot_results2D(xplot, U_pred, U_exact, T_idxs, train_losses )\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, (23,8), layout='tight')\n",
    "gs = fig.add_gridspec(2,3)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "### Plot 1 - solution profile 3D\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(X, T, U_pred.T, cmap=matplotlib.cm.rainbow,\n",
    "                       linewidth=0, antialiased=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
