{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ikbenali/ReproducibilityProject_DL/blob/main/Poisson1DNTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype  = torch.float32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "sympyTorchmodules = {'sin': torch.sin, 'cos': torch.cos}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Poisson PDE Class\n",
    "\n",
    "\\begin{align}\n",
    "u_{xx} &= f(x), \\hspace{1.3cm} x \\in \\Omega \\\\\n",
    "u(x) &= g(x), \\hspace{1.3cm} x \\in \\partial \\Omega\n",
    "\\end{align}\n",
    "\n",
    "Where: \n",
    "\n",
    "\\begin{align}\n",
    "    f(x) &= -a^{2}\\pi^{2}\\sin(a\\pi x), \\hspace{0.2cm} x \\in [0,1] \\\\\n",
    "    g(x) &= 0, \\hspace{2.3cm} x = 0,1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poisson1D:\n",
    "\n",
    "    def __init__(self, a=None):\n",
    "\n",
    "        self.setup_equations()\n",
    "        self.setup_residuals()\n",
    "\n",
    "        if a != None:\n",
    "            self.a = 1\n",
    "\n",
    "    def setup_equations(self, f_eqn=None, g_eqn=None):\n",
    "        ### Setup\n",
    "\n",
    "        # Variables/Coefficients\n",
    "        a   = sm.symbols('a'); \n",
    "\n",
    "        # PDE States\n",
    "        x   = sm.symbols('x')        # domain\n",
    "        # xbc = sm.symbols('x1:3')   # partial domain for boundary condition\n",
    "\n",
    "        u   = sm.symbols('u', cls=sm.Function)(x)\n",
    "        ux  = u.diff(x)\n",
    "        uxx = ux.diff(x)\n",
    "\n",
    "        # Forcing/External/Boundary/Initial condition functions\n",
    "        f   = sm.symbols('f', cls=sm.Function)(x)\n",
    "        g   = sm.symbols('g', cls=sm.Function)(x)\n",
    "\n",
    "        # Set up PDE_eqn\n",
    "        self.PDE_eqn = sm.Eq(uxx,f)\n",
    "\n",
    "        # Set up boundary condition\n",
    "        # bc_eq1 = sm.Piecewise((u, sm.Eq(x, xbc[0])),  (u, sm.Eq(x, xbc[1])), (0, True))\n",
    "        # bc_eq2 = sm.Piecewise((g, sm.Eq(x, xbc[0])),  (g, sm.Eq(x, xbc[1])), (0, True))\n",
    "        self.BC_eqn  = sm.Eq(u, g)\n",
    "\n",
    "        # For reuse in class\n",
    "        self.x   = x \n",
    "        # self.xbc = xbc\n",
    "        self.U = [u, ux, uxx]\n",
    "        self.f = f \n",
    "        self.g = g \n",
    "\n",
    "    def setup_residuals(self):\n",
    "        pde_residual = self.PDE_eqn.lhs - self.PDE_eqn.rhs\n",
    "        bc_residual  = self.BC_eqn.lhs  - self.BC_eqn.rhs\n",
    "\n",
    "        self.pde_residual = sm.lambdify([self.x, self.U, self.f], pde_residual, modules=sympyTorchmodules)\n",
    "        self.bc_residual  = sm.lambdify([self.x, self.U, self.g], bc_residual,  modules=sympyTorchmodules)      \n",
    "\n",
    "    def compute_gradient(self, u, x, t=None):\n",
    "            \n",
    "        ux   = torch.autograd.grad(u,  x, grad_outputs=torch.ones_like(u),  retain_graph=True, create_graph=True)[0]\n",
    "        uxx  = torch.autograd.grad(ux, x, grad_outputs=torch.ones_like(ux), create_graph=True)[0]\n",
    "\n",
    "        return torch.hstack([u, ux, uxx]).T\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define exact, source and boundary condition functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_u_exact(a,x):\n",
    "    \"\"\" \n",
    "    Exact solution\n",
    "    \"\"\"\n",
    "    u_exact = torch.sin(a*torch.pi*x)\n",
    "\n",
    "    return u_exact\n",
    "\n",
    "def f_x(a, x):\n",
    "    \"\"\"\n",
    "    Source/Forcing function\n",
    "    \"\"\"\n",
    "    fx = -(a**2)*(torch.pi**2)*torch.sin(a*torch.pi*x)\n",
    "       \n",
    "    return fx\n",
    "\n",
    "def g_x(x, xb):\n",
    "    \"\"\"\n",
    "    Boundary condition\n",
    "    \"\"\"\n",
    "    \n",
    "    ub = torch.zeros(x.size(), dtype=dtype)\n",
    "\n",
    "    xb1_idx = torch.where(x == xb[0])[0]\n",
    "    xb2_idx = torch.where(x == xb[1])[0]\n",
    "\n",
    "    ub[xb1_idx] = 0\n",
    "    ub[xb2_idx] = 0\n",
    "\n",
    "    return ub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, neurons, PDE):\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        # initialize values for nn\n",
    "        self.xin        = input_size\n",
    "        self.xout       = output_size\n",
    "        self.neurons    = neurons\n",
    "\n",
    "        # Define layers of network\n",
    "        self.layer1     = nn.Linear(input_size, neurons, dtype=dtype)\n",
    "        self.layer2     = nn.Linear(neurons, output_size, dtype=dtype)\n",
    "\n",
    "        self.layers = [self.layer1, self.layer2]\n",
    "\n",
    "        self.activation = nn.Tanh()      \n",
    "\n",
    "        # import and initialize PDE\n",
    "        if hasattr(PDE,'pde_residual'):\n",
    "            self.pde_residual = PDE.pde_residual\n",
    "        if hasattr(PDE, 'bc_residual'):\n",
    "            self.bc_residual = PDE.bc_residual\n",
    "        if hasattr(PDE, 'ic_residual'):\n",
    "            self.ic_residual = PDE.ic_residual\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # copy gradient computation\n",
    "        self.compute_pde_gradient = PDE.compute_gradient\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # Glorot Weight initalisation\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight.data)            \n",
    "            if module.bias is not None:\n",
    "                nn.init.normal_(module.bias.data)            \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def backward(self, X, U, f=None, g=None, h=None):\n",
    "\n",
    "        if X.shape[1] == 2:\n",
    "            xr = X[:, 0].view(-1,1)\n",
    "            xb = X[:, 1].view(-1,1)\n",
    "        else:\n",
    "            xr = xb = X\n",
    "\n",
    "        if len(U.shape) == 3:\n",
    "            U_x = U[0]\n",
    "            U_b = U[1]\n",
    "        else:\n",
    "            U_x = U\n",
    "            U_b = U\n",
    "\n",
    "        loss = []\n",
    "\n",
    "        if hasattr(self, 'pde_residual') and f != None:\n",
    "            residual        = self.pde_residual(xr, U_x, f).T\n",
    "            self.pde_loss   = torch.mean(residual**2)\n",
    "        \n",
    "            loss.append(self.pde_loss)\n",
    "\n",
    "        if hasattr(self, 'bc_residual') and g !=None:\n",
    "            residual        = self.bc_residual(xb, U_b, g).T\n",
    "            self.bc_loss    = torch.mean(residual**2)\n",
    "        \n",
    "            loss.append(self.bc_loss)   \n",
    "\n",
    "        if hasattr(self, 'ic_residual') and h != None:\n",
    "            residual        = self.ic_residual(X, U, h).T\n",
    "            self.ic_loss    = torch.mean(residual**2)\n",
    "            loss.append(self.ic_loss)\n",
    "\n",
    "        loss = torch.stack(loss, dim=0).sum()\n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "    def NTK(self, X1, X2):\n",
    "\n",
    "        # Get params of network\n",
    "        params = {k: v for k, v in self.named_parameters()}\n",
    "\n",
    "        PDE_K = False; BC_K = False\n",
    "\n",
    "        if X1.shape[1] == 2 and X2.shape[1] == 2:\n",
    "            xr1 = X1[:, 0].view(-1,1);   xb1 = X1[:, 1].view(-1,1)\n",
    "            xr2 = X2[:, 0].view(-1,1);   xb2 = X2[:, 1].view(-1,1)\n",
    "\n",
    "        else:\n",
    "            xr1 = xb1 = X1\n",
    "            xr2 = xb2 = X2\n",
    "\n",
    "        if hasattr(self, 'pde_residual'):\n",
    "\n",
    "            PDE_K = True;   \n",
    "\n",
    "            f = torch.zeros(xr1.size(), device=device).T\n",
    "\n",
    "            #### forward pass points with current parameters and compute gradients w.r.t interior points\n",
    "            # Compute for X\n",
    "            u_hat_x1    = self(xr1)\n",
    "            U_x1        = self.compute_pde_gradient(u_hat_x1, xr1)\n",
    "            # Compute for X'\n",
    "            u_hat_x2    = self.forward(xr2)\n",
    "            U_x2        = self.compute_pde_gradient(u_hat_x2, xr2)\n",
    "\n",
    "            # Compute LHS of PDE \n",
    "            L_u1   = self.pde_residual(xr1, U_x1, f).T\n",
    "            L_u2   = self.pde_residual(xr2, U_x2, f).T\n",
    "\n",
    "            # L_u1.retain_grad(); L_u2.retain_grad()\n",
    "\n",
    "            J_r1 = [];     J_r2 = []\n",
    "\n",
    "            for i, layer_param in enumerate(params.keys()):\n",
    "                theta    = params[layer_param]\n",
    "\n",
    "                L_u1_grad = torch.zeros(theta.shape, device=device, dtype=dtype)\n",
    "                L_u2_grad = torch.zeros(theta.shape, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "                if 'bias' in layer_param:\n",
    "                    if len(theta) == 1:\n",
    "                        L_u1_grad = torch.stack([torch.autograd.grad(L_u1_i, theta, retain_graph=True)[0] for L_u1_i in L_u1], dim=0).flatten()\n",
    "                        L_u2_grad = torch.stack([torch.autograd.grad(L_u2_i, theta, retain_graph=True)[0] for L_u2_i in L_u2], dim=0).flatten()\n",
    "                    else:\n",
    "                        L_u1_grad = torch.autograd.grad(L_u1, theta, grad_outputs=torch.ones_like(L_u1), retain_graph=True)[0].flatten()\n",
    "                        L_u2_grad = torch.autograd.grad(L_u2, theta, grad_outputs=torch.ones_like(L_u2), retain_graph=True)[0].flatten()\n",
    "                elif 'weight' in layer_param:\n",
    "                    # compute backward graph w.r.t. parameters\n",
    "                    L_u1_grad = torch.autograd.grad(L_u1, theta, grad_outputs=torch.ones_like(L_u1), retain_graph=True)[0].flatten()\n",
    "                    L_u2_grad = torch.autograd.grad(L_u2, theta, grad_outputs=torch.ones_like(L_u2), retain_graph=True)[0].flatten()\n",
    "\n",
    "                # L_u1.backward(gradient=theta, retain_graph=True)\n",
    "                # L_u2.backward(gradient=theta, retain_graph=True)\n",
    "\n",
    "                J_r1.append(L_u1_grad);    J_r2.append(L_u2_grad)\n",
    "\n",
    "                # zero out gradiants\n",
    "                # L_u1.grad.zero_();  L_u2.grad.zero_()\n",
    "            ### End backward computation over parameters\n",
    "\n",
    "            J_r1  = torch.stack(J_r1, dim=0).T;    J_r2 = torch.stack(J_r2, dim=0).T\n",
    "            \n",
    "            # compute NTK matrix for PDE residual\n",
    "            self.K_rr       = J_r1 @ J_r2.T\n",
    "            self.lambda_rr  = torch.linalg.eigvals(self.K_rr)\n",
    "\n",
    "            #end\n",
    "\n",
    "        if hasattr(self, 'bc_residual'):\n",
    "\n",
    "            BC_K = True \n",
    "            g = torch.zeros(xb1.size(), device=device).T\n",
    "\n",
    "            #### forward pass points with current parameters and compute gradients w.r.t boundary points\n",
    "            # Compute for X\n",
    "            u_hat_xb1    = self.forward(xb1)\n",
    "            U_xb1        = self.compute_pde_gradient(u_hat_xb1, xb1)\n",
    "            # Compute for X'\n",
    "            u_hat_xb2    = self.forward(xb2)\n",
    "            U_xb2        = self.compute_pde_gradient(u_hat_xb2, xb2)\n",
    "            \n",
    "            u1   = self.bc_residual(xb1, U_xb1, g).T.flatten()\n",
    "            u2   = self.bc_residual(xb2, U_xb2, g).T.flatten()\n",
    "\n",
    "            u1.retain_grad();     u2.retain_grad()\n",
    "\n",
    "\n",
    "            J_u1 = [];     J_u2 = []\n",
    "\n",
    "            for i, layer_param in enumerate(params.keys()):\n",
    "                theta    = params[layer_param]\n",
    "\n",
    "                u1_grad = torch.zeros(theta.shape, device=device, dtype=dtype)\n",
    "                u2_grad = torch.zeros(theta.shape, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "                if 'bias' in layer_param:\n",
    "                    if len(theta) == 1:\n",
    "                        u1_grad = torch.stack([torch.autograd.grad(u1_i, theta, retain_graph=True)[0] for u1_i in u1], dim=0).flatten()\n",
    "                        u2_grad = torch.stack([torch.autograd.grad(u2_i, theta, retain_graph=True)[0] for u2_i in u2], dim=0).flatten()\n",
    "                    else:\n",
    "                        u1_grad = torch.autograd.grad(u1, theta, grad_outputs=torch.ones_like(u1), retain_graph=True)[0].flatten()\n",
    "                        u2_grad = torch.autograd.grad(u2, theta, grad_outputs=torch.ones_like(u2), retain_graph=True)[0].flatten()\n",
    "                elif 'weight' in layer_param:\n",
    "                    # compute backward graph w.r.t. parameters\n",
    "                    u1_grad = torch.autograd.grad(u1, theta, grad_outputs=torch.ones_like(u1), retain_graph=True)[0].flatten()\n",
    "                    u2_grad = torch.autograd.grad(u2, theta, grad_outputs=torch.ones_like(u2), retain_graph=True)[0].flatten()\n",
    "\n",
    "                # u1.backward(gradient=theta, retain_graph=True)\n",
    "                # u2.backward(gradient=theta, retain_graph=True)\n",
    "\n",
    "                J_u1.append(u1_grad);    J_u2.append(u2_grad)\n",
    "\n",
    "                # zero out gradiants\n",
    "                # u1.grad.zero_();  u2.grad.zero_()\n",
    "            ### End backward computation over parameters\n",
    "            \n",
    "            J_u1  = torch.stack(J_u1, dim=0).T;    J_u2 = torch.stack(J_u2, dim=0).T\n",
    "\n",
    "            self.K_uu       = J_u1 @ J_u2.T\n",
    "            self.lambda_uu  = torch.linalg.eigvals(self.K_uu)\n",
    "            \n",
    "        if PDE_K and BC_K:\n",
    "\n",
    "            K1 = torch.vstack((J_u1,   J_r1))\n",
    "            K2 = torch.hstack((J_u2.T, J_r2.T))\n",
    "\n",
    "            self.K = K1 @ K2\n",
    "\n",
    "            self.lambda_K = torch.linalg.eigvals(self.K)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup PDE Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup PDE Equation\n",
    "a   = 2\n",
    "PDE = Poisson1D(a)\n",
    "\n",
    "# Define PDE domain\n",
    "X_0,X_N = 0.,1.\n",
    "X_bc  = [X_0, X_N]\n",
    "\n",
    "N  = 500\n",
    "dx = (X_N - X_0) / N\n",
    "\n",
    "Xr = torch.linspace(X_0, X_N, N, dtype=dtype, device=device, requires_grad=True).view(-1,1)\n",
    "Xb = torch.randint(0, 2, (N,1),  dtype=dtype, device=device, requires_grad=True)\n",
    "\n",
    "X  = torch.hstack((Xr, Xb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup PINN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup PINN parameters\n",
    "\n",
    "Nr      = 100\n",
    "Nb      = 100\n",
    "rand_sampler = RandomSampler(X, replacement=True)\n",
    "XTrain       = DataLoader(X, Nr ,sampler=rand_sampler)\n",
    "\n",
    "size          = len(XTrain.dataset)\n",
    "learning_rate = 1e-3\n",
    "epochs        = int(10e3)\n",
    "\n",
    "# net parameters\n",
    "input_size  = 1\n",
    "output_size = 1\n",
    "neurons     = 100\n",
    "net         = PINN(input_size, output_size, neurons, PDE); net.to(device)\n",
    "\n",
    "loss_fn   = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), learning_rate)\n",
    "# optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### TRAIN LOOP\n",
    "train_losses = []\n",
    "\n",
    "# NTK computation\n",
    "compute_NTK          = False\n",
    "compute_NTK_interval = 1\n",
    "store_NTK    = True\n",
    "eig_K        = []\n",
    "eig_K_uu     = []\n",
    "eig_K_rr     = []\n",
    "\n",
    "# Auto Mixed Precision settings\n",
    "use_amp = False\n",
    "scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    net.train()\n",
    "\n",
    "    epoch_loss   = 0.0\n",
    "\n",
    "    for i, x in enumerate(XTrain):\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xr = x[:,0].view(-1,1).to(device); xb = x[:,1].view(-1,1).to(device)\n",
    "\n",
    "        ### INTERIOR DOMAIN\n",
    "        # make prediction w.r.t. interior points\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "\n",
    "            ### Predict interior points\n",
    "            u_hat_x   = net(xr)\n",
    "        \n",
    "            # determine gradients w.r.t interior points\n",
    "            U_x       =  net.compute_pde_gradient(u_hat_x, xr)\n",
    "\n",
    "            ### BOUNDARY DOMAIN\n",
    "            u_hat_xb    = net(xb)\n",
    "\n",
    "            # determine gradients w.r.t boundary points\n",
    "            U_xb       =  net.compute_pde_gradient(u_hat_xb, xb)\n",
    "            \n",
    "            # Compute forcing/source function\n",
    "            fx = f_x(a, xr).T.to(device)\n",
    "\n",
    "            # compute boundary condition\n",
    "            gx = g_x(xb, X_bc).T.to(device)\n",
    "\n",
    "            # Stack\n",
    "            U = torch.stack((U_x, U_xb), dim=0)\n",
    "\n",
    "            ## Backward step\n",
    "            net.backward(x, U, fx, gx)\n",
    "            epoch_loss += net.loss.item()\n",
    "            if i == len(XTrain) - 1:\n",
    "                x_prime  = x.clone()\n",
    "\n",
    "        # Do optimisation step\n",
    "        if use_amp:\n",
    "            scaler.scale(net.loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "    ### END Batch loop\n",
    "\n",
    "    # Compute NTK\n",
    "    if epoch > 0:\n",
    "        if (epoch % compute_NTK_interval == 0 or epoch == epochs - 1) and compute_NTK:\n",
    "\n",
    "            net.eval()\n",
    "\n",
    "            x_          = x.detach().clone().requires_grad_()\n",
    "            x_prime_    = x_prime.detach().clone().requires_grad_()\n",
    "\n",
    "            net.NTK(x_, x_prime_)\n",
    "\n",
    "            if store_NTK:\n",
    "                eig_K.append(net.lambda_K)\n",
    "                eig_K_uu.append(net.lambda_uu)\n",
    "                eig_K_rr.append(net.lambda_rr)\n",
    "\n",
    "    train_losses.append(epoch_loss / len(XTrain))\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == epochs - 1: \n",
    "        print(f\"Epoch: {epoch:4d}     loss: {train_losses[-1]:5f}\")\n",
    "### END training loop\n",
    "\n",
    "if compute_NTK:\n",
    "    # reformat eigenvalue of NTK matrices\n",
    "    eig_K       = torch.stack(eig_K, dim=-1)\n",
    "    eig_K_uu    = torch.stack(eig_K_uu, dim=-1)\n",
    "    eig_K_rr    = torch.stack(eig_K_rr, dim=-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot = torch.linspace(X_0, X_N, N, requires_grad=True, dtype=dtype).view(-1,1).to(device)\n",
    "\n",
    "# compute exact solution\n",
    "u_exact = f_u_exact(a, xplot)\n",
    "u_pred  = net(xplot)\n",
    "\n",
    "xplot   = xplot.cpu().detach().numpy()\n",
    "u_exact = u_exact.cpu().detach().numpy()\n",
    "u_pred  = u_pred.cpu().detach().numpy()\n",
    "\n",
    "### PLOT Prediction accuracy and training loss\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(23,6))\n",
    "\n",
    "# predict\n",
    "axs[0].plot(xplot, u_exact, label='$u_{exact}$')\n",
    "axs[0].plot(xplot, u_pred, label='$u_{pred}$')\n",
    "axs[0].legend()\n",
    "axs[0].set_ylabel(r'$u$')\n",
    "axs[0].set_xlabel(r'$x$')\n",
    "\n",
    "\n",
    "axs[1].semilogy(train_losses)\n",
    "axs[1].set_ylabel(r'loss per epoch')\n",
    "axs[1].set_xlabel(r'$Epoch$')\n",
    "\n",
    "\n",
    "if compute_NTK:\n",
    "        eig_K_plot    = np.real(eig_K.detach().cpu().numpy())\n",
    "        eig_K_uu_plot = np.real(eig_K_uu.detach().cpu().numpy())\n",
    "        eig_K_rr_plot = np.real(eig_K_rr.detach().cpu().numpy())\n",
    "\n",
    "        ### PLOT Eigenvalue of NTK matrices\n",
    "        fig, axs = plt.subplots(1,3, figsize=(23,6))\n",
    "\n",
    "        axs[0].semilogx(eig_K_plot[:,-1],      label=r'$\\lambda_{K}$');     axs[0].set_title('Eigenvalue of K')\n",
    "        axs[1].semilogx(eig_K_uu_plot[:,-1],   label=r'$\\lambda_{uu}$');    axs[1].set_title('Eigenvalue of {}'.format(r\"$K_{uu}$\"))\n",
    "        axs[2].semilogx(eig_K_rr_plot[:,-1],   label=r'$\\lambda_{rr}$');    axs[2].set_title('Eigenvalue of {}'.format(r\"$K_{rr}$\"))\n",
    "\n",
    "        for ax in axs:\n",
    "                ax.legend()\n",
    "                ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "                ax.set_ylabel(r'$\\lambda$')\n",
    "                ax.set_xlabel(r'$Index$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
